{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"}},"cells":[{"cell_type":"markdown","metadata":{"id":"eLwmFZfKxLi4"},"source":["### SkLearn Implementation"]},{"cell_type":"code","metadata":{"id":"bUsYm9wjxLi1"},"source":["## SkLearn# Collection of string documents\n","\n","corpus = [\n","     'this is the first document',\n","     'this document is the second document',\n","     'and this is the third one',\n","     'is this the first document',\n","]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Np4dfQOkxLi4"},"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","vectorizer = TfidfVectorizer()\n","vectorizer.fit(corpus)\n","skl_output = vectorizer.transform(corpus)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-7Om8YpYxLi6","outputId":"0a3bd0f5-4424-4400-944f-4482a80bd799"},"source":["# sklearn feature names, they are sorted in alphabetic order by default.\n","\n","print(vectorizer.get_feature_names())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dTKplK96xLi-","outputId":"53722fa2-6756-4aa0-f179-37b578bb6890"},"source":["# Here we will print the sklearn tfidf vectorizer idf values after applying the fit method\n","# After using the fit function on the corpus the vocab has 9 words in it, and each has its idf value.\n","\n","print(vectorizer.idf_)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[1.91629073 1.22314355 1.51082562 1.         1.91629073 1.91629073\n"," 1.         1.91629073 1.        ]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-CTiWHygxLjA","outputId":"8d5a9cde-2c29-4afe-f7b4-1547e88dba4f"},"source":["# shape of sklearn tfidf vectorizer output after applying transform method.\n","\n","skl_output.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(4, 9)"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"bDKEpbA-xLjD","outputId":"87dafd65-5313-443f-8c6e-1b05cc8c2543"},"source":["# sklearn tfidf values for first line of the above corpus.\n","# Here the output is a sparse matrix\n","\n","print(skl_output[0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["  (0, 8)\t0.38408524091481483\n","  (0, 6)\t0.38408524091481483\n","  (0, 3)\t0.38408524091481483\n","  (0, 2)\t0.5802858236844359\n","  (0, 1)\t0.46979138557992045\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3QWo34hexLjF","outputId":"cdc04e08-989f-4bdc-dd7f-f1c82a9f90be"},"source":["# sklearn tfidf values for first line of the above corpus.\n","# To understand the output better, here we are converting the sparse output matrix to dense matrix and printing it.\n","# Notice that this output is normalized using L2 normalization. sklearn does this by default.\n","\n","print(skl_output[0].toarray())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n","  0.38408524 0.         0.38408524]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qfIwx5LzxLjI"},"source":["### Custom implementation"]},{"cell_type":"code","source":["from collections import Counter\n","from tqdm import tqdm\n","from scipy.sparse import csr_matrix\n","import math\n","from math import log\n","import operator\n","import numpy as np\n","import warnings\n","warnings.filterwarnings(\"ignore\")"],"metadata":{"id":"mBgYDmlGNaar"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#importing sklearn for comparison\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","vectorizer = TfidfVectorizer()\n","vectorizer.fit(corpus)\n","skl_output = vectorizer.transform(corpus)"],"metadata":{"id":"VuJ3hsb_ZotY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["corpus = [\n","     'this is the first document',\n","     'this document is the second document',\n","     'and this is the third one',\n","     'is this the first document',]"],"metadata":{"id":"B5f2PT4Jd4BJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def fit(text):\n","  unique_w = set()\n","  if isinstance(text, (list,)):\n","    for row in text:\n","      for w in row.split(\" \"):\n","        if len(w) <2:\n","          continue\n","        unique_w.add(w)\n","    unique_w = sorted(list(unique_w))\n","    voc = {j:i for i,j in enumerate(unique_w)}\n","    return voc\n","  else:\n","    print(\"Pass the text as a list - fit\")"],"metadata":{"id":"3iNjAcXdN4ha"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('fit function - custom implementation: \\n',fit(corpus).keys())\n","print('fit function - sklearn implementation: \\n',vectorizer.get_feature_names())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_Qrb2wVWZM8M","outputId":"2efce80e-ac9c-4b92-fc25-bb229bc3fdea"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["fit function - custom implementation: \n"," dict_keys(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this'])\n","fit function - sklearn implementation: \n"," ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"]}]},{"cell_type":"code","source":["def get_BoW(text, voc):\n","  rows = []\n","  cols = []\n","  vals = []\n","  if isinstance(text, (list,)):\n","    for idx, row in enumerate(text):\n","      w_freq = dict(Counter(row.split()))\n","      for w, freq in w_freq.items():\n","        if len(w) < 2:\n","          continue\n","        col_idx = voc.get(w, -1)\n","        if col_idx != -1:\n","          rows.append(idx)\n","          cols.append(col_idx)\n","          vals.append(freq)\n","    return csr_matrix((vals, (rows, cols)), shape=(len(text),len(voc)))\n","  else:\n","    print(\"Pass the text as a list - transform\")"],"metadata":{"id":"fJDoyvFNRlla"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def myTF(corpus):\n","  doc_count = len(get_BoW(corpus,fit(corpus)).toarray())\n","  vocab_len = len(fit(corpus))\n","  TF = csr_matrix(([0],([0],[0])),shape=(doc_count,vocab_len))\n","  BoW = get_BoW(corpus,fit(corpus)).toarray()\n","  for i,j in enumerate(BoW):\n","    term_doc_count = j.sum()\n","    for m,n in enumerate(j):\n","      if n == 0:\n","        continue\n","      TF += csr_matrix(([n/term_doc_count],([i],[m])),shape=(doc_count,vocab_len))\n","  return(TF.toarray())"],"metadata":{"id":"EVcTYzTqui_i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def myIDF(text):\n","  idf = {}\n","  row_cnt = 0\n","  row_with_w = 0\n","  w_tfidf = {}\n","  if isinstance(text, (list,)):\n","    for row in text:\n","      for w in row.split(\" \"):\n","        for row1 in text:\n","          if w in row1.split(\" \"):\n","            row_with_w += 1\n","        w_tfidf.update({w:row_with_w})\n","        row_with_w = 0\n","      row_cnt += 1\n","    for row in text:\n","      for w in row.split(\" \"):\n","        if len(w) <2:\n","          continue\n","    for key,value in fit(text).items():\n","      idf.update({key:round(1 + log((1 + row_cnt)/(1 + w_tfidf.get(key))),8)})\n","  else:\n","    print(\"Pass the text as a list - tfidf\")\n","  return([value for key,value in idf.items()])"],"metadata":{"id":"x1db7bQH0dP0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Custom IDF: \\n',myIDF(corpus))\n","print('sklearn IDF: \\n',vectorizer.idf_)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HPtubSzQewbI","outputId":"e3121e56-4132-4141-cb5e-a6f262cb9bd6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Custom IDF: \n"," [1.91629073, 1.22314355, 1.51082562, 1.0, 1.91629073, 1.91629073, 1.0, 1.91629073, 1.0]\n","sklearn IDF: \n"," [1.91629073 1.22314355 1.51082562 1.         1.91629073 1.91629073\n"," 1.         1.91629073 1.        ]\n"]}]},{"cell_type":"code","source":["def transform(corpus):\n","  TF = myTF(corpus)\n","  IDF = myIDF(corpus)\n","  doc_count = len(get_BoW(corpus,fit(corpus)).toarray())\n","  vocab_len = len(fit(corpus))\n","  TFIDF_init = csr_matrix(([0],([0],[0])),shape=(doc_count,vocab_len))\n","  TFIDF_fnl = csr_matrix(([0],([0],[0])),shape=(doc_count,vocab_len))\n","  sum_of_sq = 0\n","\n","  for i,j in enumerate(IDF):\n","    for m,n in enumerate(TF):\n","      for x,y in enumerate(n):\n","        if i == x:\n","          TFIDF_init += csr_matrix(([j*y],([m],[x])),shape=(doc_count,vocab_len))\n","\n","  for i_1,j_1 in enumerate(TFIDF_init.toarray()):\n","    sum_of_sq = sum([m_1**2 for m_1 in j_1])**0.5\n","    for x_1,y_1 in enumerate(j_1):\n","      TFIDF_fnl += csr_matrix(([y_1/sum_of_sq],([i_1],[x_1])),shape=(doc_count,vocab_len))\n","    sum_of_sq = 0\n","  return(TFIDF_fnl)"],"metadata":{"id":"nV9OLMyT-4c4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Diamension of custom TFIDF matrix: ',transform(corpus).shape)\n","print('Diamension of sklearn TFIDF matrix: ',skl_output.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eAJGWYWVd1gz","outputId":"f74760f6-545d-4d4a-e7da-8421d85331c6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Diamension of custom TFIDF matrix:  (4, 9)\n","Diamension of sklearn TFIDF matrix:  (4, 9)\n"]}]},{"cell_type":"code","source":["print('Custom TFIDF: \\n',transform(corpus)[0])\n","print('sklearn TFIDF: \\n',skl_output[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_WpLTjyA69r3","outputId":"52ecaad2-a00e-48c9-fde0-b188cfad8383"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Custom TFIDF: \n","   (0, 1)\t0.46979138558088085\n","  (0, 2)\t0.5802858228626505\n","  (0, 3)\t0.3840852413282814\n","  (0, 6)\t0.3840852413282814\n","  (0, 8)\t0.3840852413282814\n","sklearn TFIDF: \n","   (0, 8)\t0.38408524091481483\n","  (0, 6)\t0.38408524091481483\n","  (0, 3)\t0.38408524091481483\n","  (0, 2)\t0.5802858236844359\n","  (0, 1)\t0.46979138557992045\n"]}]},{"cell_type":"markdown","metadata":{"id":"51j_OtqAxLjL"},"source":["Implement max features functionality"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TCLpCPO_qodD","outputId":"aa73d29d-e24b-4d7f-f7c6-f443c6690318"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import pickle\n","with open('/content/drive/My Drive/Applied AI Course/Assignments/cleaned_strings', 'rb') as f:\n","    corpus = pickle.load(f)\n","print(\"Number of documents in corpus = \",len(corpus))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s0kXLALSpoNQ","outputId":"12f630de-9e58-4735-9598-76c9062166ed"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of documents in corpus =  746\n"]}]},{"cell_type":"code","source":["from collections import Counter\n","from tqdm import tqdm\n","from scipy.sparse import csr_matrix\n","import math\n","from math import log\n","import operator\n","import numpy as np\n","import os\n","import warnings\n","warnings.filterwarnings(\"ignore\")"],"metadata":{"id":"r2YOHf7so7HS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#importing sklearn for comparison\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","vectorizer = TfidfVectorizer()\n","vectorizer.fit(corpus)\n","skl_output = vectorizer.transform(corpus)"],"metadata":{"id":"reEwsVPOo7HT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def order_words(text):\n","  unique_w = set()\n","  if isinstance(text, (list,)):\n","    for row in text:\n","      for w in row.split(\" \"):\n","        if len(w) <2:\n","          continue\n","        unique_w.add(w)\n","    unique_w = sorted(list(unique_w))\n","    voc = {j:i for i,j in enumerate(unique_w)}\n","    return voc\n","  else:\n","    print(\"Pass the text as a list - order_words\")"],"metadata":{"id":"17eFpxDMITll"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_BoW(text, voc):\n","  rows = []\n","  cols = []\n","  vals = []\n","  if isinstance(text, (list,)):\n","    for idx, row in enumerate(text):\n","      w_freq = dict(Counter(row.split()))\n","      for w, freq in w_freq.items():\n","        if len(w) < 2:\n","          continue\n","        col_idx = voc.get(w, -1)\n","        if col_idx != -1:\n","          rows.append(idx)\n","          cols.append(col_idx)\n","          vals.append(freq)\n","    return csr_matrix((vals, (rows, cols)), shape=(len(text),len(voc)))\n","  else:\n","    print(\"Pass the text as a list - transform\")"],"metadata":{"id":"j0rv8kRAo7HV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def myTF(corpus,BoW,doc_count_pub,vocab_len_pub):\n","  doc_count = doc_count_pub\n","  vocab_len = vocab_len_pub\n","  TF = csr_matrix(([0],([0],[0])),shape=(doc_count,vocab_len))\n","  BoW_TF = BoW.toarray()\n","  for i,j in enumerate(BoW_TF):\n","    term_doc_count = j.sum()\n","    for m,n in enumerate(j):\n","      if n == 0:\n","        continue\n","      TF += csr_matrix(([n/term_doc_count],([i],[m])),shape=(doc_count,vocab_len))\n","  return(TF.toarray())"],"metadata":{"id":"v-WLRAOuo7HV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def IDF_vocab(text,I_or_v):\n","  # Initialize idf variable as dictionary to add idf values iteratively\n","  idf = {}\n","  # Initialize a counter variable to count number of documents in the collection\n","  row_cnt = 0\n","  # Initialize a counter variable to count documents having current term\n","  row_with_w = 0\n","  # Initialize a dictionary to store number of documents for all the terms in the collection\n","  w_tfidf = {}\n","  # Initialize a dictionary to store the vocabulary with the IDF values\n","  vocab_IDF = {}\n","  # Check if the collection is a list\n","  if isinstance(text, (list,)):\n","    # For loop to iterate through all the documents in the collection to find\n","    # number of documents with the term\n","    for row in text:\n","      # For loop to split the terms in the document and iterate through all the terms\n","      # in the documents to find number of documents with the term\n","      for w in row.split(\" \"):\n","        # For loop to iterate through each documents in the collection\n","        # The above two for loop fixes a term and the below for loop will run to find\n","        # and count documents with matching term\n","        for row1 in text:\n","          # Split the documents in the collection and check if the document\n","          # has the term (w) fix by the the for loop one above\n","          if w in row1.split(\" \"):\n","            # Count and store the number of documents with the term (w) fix by the\n","            # for loop one above\n","            row_with_w += 1\n","        # For each term \"w\" in the document, store the term and the count of documents\n","        # having that term, to the dictionary \"w_tfidf\"\n","        w_tfidf.update({w:row_with_w})\n","        # Reset the counter variable that store number of documents with term \"w\"\n","        row_with_w = 0\n","      # Increment the document counter to count the total number of documents in the collection\n","      row_cnt += 1\n","    # The below for loop is to generate the IDF dictionary that calculate the\n","    # IDF value from the number of documents in the collection and the number\n","    # of documents that has a given term both are calculated in the above steps.\n","    # The for loop will iterate through all the terms in the vocabulary created by the\n","    # function \"order_words\" and create IDF for all the terms in the collection.\n","    for key,value in order_words(text).items():\n","      # Create and store IDF values for all the terms in the collection\n","\n","#      This line of code calculate the IDF value for all the terms in the collection.\n","#      idf is a dictionary and update function add values to the dictionary. It accepts a key and value in curley brackets.\n","#      \"key\" is the term from the vocabulary which is generated by the function \"order_words\".\n","#      round is the round function used to round the output of IDF calculation to 8 digits to match the output of sklearn.\n","#      Next is the equation for IDF value with updates for sklearn. The updated formula for IDF in sklearn is 1+log((1+Total number of documents in collection)/(1+Number of documents with term t in it))\n","#      Here, \"row_cnt\" is the total number of documents in collection and \"w_tfidf.get(key)\" is the number of documents with term t in it.\n","#      The \"row_cnt\" is updated by iterating through all the documents in the collection. The \"w_tfidf\" is a dictionary that contains the terms and corresponding numbers for the number of documents with term t that is updated by two for loops in the previous steps.\n","\n","      idf.update({key:round(1 + log((1 + row_cnt)/(1 + w_tfidf.get(key))),8)})\n","\n","  # else to show a message if the collection is not given as list (list of documents)\n","  else:\n","    # printing message to show the given input is not list\n","    print(\"Pass the text as a list - tfidf\")\n","  # Sorting the IDF values and taking the top 50 items\n","  idf = sorted(idf.items(),key=lambda x:x[1],reverse=True)[:50]\n","  # The sorting returns a list and the below code converts it to a dictionary\n","  idf = dict(idf)\n","  # Sorting the IDF dictionary and get only terms - removing IDF values for vocabulary\n","  vocab = sorted(idf.keys())\n","  # Adding index numbers to the vocabulary\n","  vocab = {j:i for i,j in enumerate(vocab)}\n","  # Iterating through vocabulary and adding  terms and IDF values to a new dictionary\n","  for key,value in vocab.items():\n","    # Updating the new dictionary with terms and IDF values\n","    vocab_IDF.update({key:idf.get(key)})\n","  #The below \"if-else\" loop is to return vocabulary, IDF and both based on the parameters in the calling function\n","  if I_or_v == 'Iv':\n","    # Return vocabulary with IDF values in dictionary - sorted by words\n","    return(vocab_IDF)\n","  elif I_or_v == 'I':\n","    # Return only IDF values in list - sorted by words\n","    return([value for key,value in vocab_IDF.items()])\n","  elif I_or_v == 'v':\n","    # Return vocabulary with index ID in dictionary - sorted by words\n","    return(vocab)"],"metadata":{"id":"Kh04Rl_Lo7HW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def fit(corpus):\n","  global IDF_pub\n","  global TF_pub\n","  global doc_count_pub\n","  global vocab_len_pub\n","  global BoW\n","  global fit_corpus\n","  fit_corpus = corpus\n","  vocab_return = IDF_vocab(corpus,'v')\n","  BoW = get_BoW(corpus,vocab_return)\n","  doc_count_pub = len(BoW.toarray())\n","  vocab_len_pub = len(vocab_return)\n","  TF_pub = myTF(corpus,BoW,doc_count_pub,vocab_len_pub)\n","  IDF_pub = IDF_vocab(corpus,'I')\n","  return(vocab_return)"],"metadata":{"id":"QefmYPXfiF9c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def myIDF(corpus):\n","  return(IDF_vocab(corpus,'I'))\n","def vocab_with_IDF(corpus):\n","  return(IDF_vocab(corpus,'Iv'))\n","\n","print('fit function returning 50 terms with top IDF values: \\n',fit(corpus))\n","print('Top 50 IDF values sorted by ascending order of words: \\n',myIDF(corpus))\n","print('Vocabulary with IDF values for top 50 words: \\n',vocab_with_IDF(corpus))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M6AVp6tQcwJK","outputId":"9bf80ac6-db71-4bd8-8de6-119e246ac62b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["fit function returning 50 terms with top IDF values: \n"," {'aailiyah': 0, 'abandoned': 1, 'abroad': 2, 'abstruse': 3, 'academy': 4, 'accents': 5, 'accessible': 6, 'acclaimed': 7, 'accolades': 8, 'accurate': 9, 'accurately': 10, 'achille': 11, 'ackerman': 12, 'actions': 13, 'adams': 14, 'add': 15, 'added': 16, 'admins': 17, 'admiration': 18, 'admitted': 19, 'adrift': 20, 'adventure': 21, 'aesthetically': 22, 'affected': 23, 'affleck': 24, 'afternoon': 25, 'aged': 26, 'ages': 27, 'agree': 28, 'agreed': 29, 'aimless': 30, 'aired': 31, 'akasha': 32, 'akin': 33, 'alert': 34, 'alike': 35, 'allison': 36, 'allow': 37, 'allowing': 38, 'alongside': 39, 'amateurish': 40, 'amaze': 41, 'amazed': 42, 'amazingly': 43, 'amusing': 44, 'amust': 45, 'anatomist': 46, 'angel': 47, 'angela': 48, 'angelina': 49}\n","Top 50 IDF values sorted by ascending order of words: \n"," [6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918, 6.922918]\n","Vocabulary with IDF values for top 50 words: \n"," {'aailiyah': 6.922918, 'abandoned': 6.922918, 'abroad': 6.922918, 'abstruse': 6.922918, 'academy': 6.922918, 'accents': 6.922918, 'accessible': 6.922918, 'acclaimed': 6.922918, 'accolades': 6.922918, 'accurate': 6.922918, 'accurately': 6.922918, 'achille': 6.922918, 'ackerman': 6.922918, 'actions': 6.922918, 'adams': 6.922918, 'add': 6.922918, 'added': 6.922918, 'admins': 6.922918, 'admiration': 6.922918, 'admitted': 6.922918, 'adrift': 6.922918, 'adventure': 6.922918, 'aesthetically': 6.922918, 'affected': 6.922918, 'affleck': 6.922918, 'afternoon': 6.922918, 'aged': 6.922918, 'ages': 6.922918, 'agree': 6.922918, 'agreed': 6.922918, 'aimless': 6.922918, 'aired': 6.922918, 'akasha': 6.922918, 'akin': 6.922918, 'alert': 6.922918, 'alike': 6.922918, 'allison': 6.922918, 'allow': 6.922918, 'allowing': 6.922918, 'alongside': 6.922918, 'amateurish': 6.922918, 'amaze': 6.922918, 'amazed': 6.922918, 'amazingly': 6.922918, 'amusing': 6.922918, 'amust': 6.922918, 'anatomist': 6.922918, 'angel': 6.922918, 'angela': 6.922918, 'angelina': 6.922918}\n"]}]},{"cell_type":"code","source":["def transform(corpus):\n","  TF = TF_pub\n","  IDF = IDF_pub\n","  doc_count = doc_count_pub\n","  vocab_len = vocab_len_pub\n","  fit_corp = fit_corpus\n","  TFIDF_1 = csr_matrix(([0],([0],[0])),shape=(doc_count,vocab_len))\n","  TFIDF_2 = csr_matrix(([0],([0],[0])),shape=(doc_count,vocab_len))\n","  sum_of_sq = 0\n","\n","  for i,j in enumerate(IDF):\n","    for m,n in enumerate(TF):\n","      for x,y in enumerate(n):\n","        if i == x:\n","          TFIDF_1 += csr_matrix(([j*y],([m],[x])),shape=(doc_count,vocab_len))\n","\n","  for i_1,j_1 in enumerate(TFIDF_1.toarray()):\n","    sum_of_sq = sum([m_1**2 for m_1 in j_1])**0.5\n","    for x_1,y_1 in enumerate(j_1):\n","      if math.isnan(y_1/sum_of_sq):\n","        tfidf_sum = 0.0\n","      else:\n","        tfidf_sum = y_1/sum_of_sq\n","      TFIDF_2 += csr_matrix(([tfidf_sum],([i_1],[x_1])),shape=(doc_count,vocab_len))\n","    sum_of_sq = 0\n","  if corpus == fit_corp:\n","    return(TFIDF_2)\n","  elif (len(fit_corp) > 1) & (len(corpus) == 1):\n","    for i in range(0,len(fit_corp)):\n","      if fit_corp[i] == corpus[0]:\n","        return(TFIDF_2[i])\n","  else:\n","    print('The dimension of fit data and transform data are not matching')"],"metadata":{"id":"Th74Tk9Bo7HX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fit(corpus)\n","print('transform function returning sparse matrix, converting to array: \\n',transform(corpus).toarray())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sT6kcYg9flSG","outputId":"e70a03e3-9c9b-4306-ae0e-ba26de86886b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["transform function returning sparse matrix, converting to array: \n"," [[0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," ...\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]]\n"]}]},{"cell_type":"code","source":["fit(corpus)\n","print('Output of a single document in the collection: \\n',transform([corpus[500]]).toarray())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ulMlUwf0KrNX","outputId":"6522059e-9a6d-455c-8c74-533e95937b47"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Output of a single document in the collection: \n"," [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n","  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n","  1. 0.]]\n"]}]}]}